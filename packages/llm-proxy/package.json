{
    "name": "llm-proxy",
    "version": "1.0.0",
    "description": "LLM inference proxy with cost tracking and rate limiting",
    "main": "dist/index.js",
    "scripts": {
        "build": "tsc",
        "start": "node dist/index.js",
        "postinstall": "npm run build"
    },
    "keywords": [
        "llm",
        "proxy",
        "cost-tracking",
        "vercel-ai"
    ],
    "author": "",
    "license": "MIT",
    "dependencies": {
        "@ai-sdk/anthropic": "^1.0.0",
        "@ai-sdk/openai": "^1.0.0",
        "ai": "^3.4.33",
        "dotenv": "^16.3.1",
        "express": "^4.18.2",
        "openai": "^4.104.0",
        "zod": "^3.22.4"
    },
    "devDependencies": {
        "@types/express": "^4.17.21",
        "@types/node": "^20.10.5",
        "tsx": "^4.7.0",
        "typescript": "^5.3.3"
    }
}

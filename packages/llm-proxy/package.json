{
    "name": "llm-proxy",
    "version": "1.0.0",
    "description": "LLM inference proxy with cost tracking and rate limiting",
    "main": "dist/index.js",
    "scripts": {
        "build": "tsc",
        "start": "node dist/index.js",
        "postinstall": "npm run build"
    },
    "keywords": [
        "llm",
        "proxy",
        "cost-tracking",
        "vercel-ai"
    ],
    "author": "",
    "license": "MIT",
    "dependencies": {
        "@ai-sdk/anthropic": "^2.0.41",
        "@ai-sdk/deepseek": "^1.0.27",
        "@ai-sdk/fireworks": "^1.0.27",
        "@ai-sdk/openai": "^2.0.64",
        "@ai-sdk/togetherai": "^1.0.27",
        "@types/uuid": "^10.0.0",
        "ai": "^5.0.88",
        "axios": "^1.13.2",
        "dotenv": "^16.3.1",
        "express": "^4.18.2",
        "fs-extra": "^11.3.2",
        "openai": "^4.104.0",
        "uuid": "^13.0.0",
        "zod": "^3.22.4",
        "zod-to-json-schema": "^3.24.6"
    },
    "devDependencies": {
        "@types/express": "^4.17.21",
        "@types/node": "^20.10.5",
        "tsx": "^4.7.0",
        "typescript": "^5.3.3"
    }
}
